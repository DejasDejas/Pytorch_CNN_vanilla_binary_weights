{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Omniglot_binary_classification_V2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDj3yJsyKr6JrO3DNDRmay"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XSftjtFg2HYJ","colab_type":"text"},"source":["# Mount Drive:"]},{"cell_type":"code","metadata":{"id":"y6M8IWW11q7n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"a461cad6-a212-498c-ea72-eca0c9f92c44","executionInfo":{"status":"ok","timestamp":1587043849860,"user_tz":-120,"elapsed":1118,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["#Import drive\n","from google.colab import drive\n","#Mount Google Drive\n","drive.mount(\"/content/drive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U6WyylOx16wT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"98486708-7298-4066-a7b0-74a63dc8f4aa","executionInfo":{"status":"ok","timestamp":1587043854535,"user_tz":-120,"elapsed":2980,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["import os\n","os.chdir('drive/My Drive/Work/Thesis_Julien_Dejasmin/Work/code/Binary_activations_V2/MNIST_Binary_V2')\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["config.py   distributions  README.md\t     trained_models\n","data\t    experiments    requirements.txt  utils\n","DataLoader  __pycache__    results\t     visualize\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j2esE3JR2RH0","colab_type":"text"},"source":["# Import:"]},{"cell_type":"code","metadata":{"id":"v2rwS8K72G7R","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torchvision.transforms import Compose, ToTensor, Normalize\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from functools import partial\n","\n","from utils.models import get_my_model_Omniglot, fetch_last_checkpoint_model_filename\n","from DataLoader.dataLoaders import get_omniglot_dataloaders_classification\n","from utils.training import run, evaluate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4XBDOH3L2VBC","colab_type":"text"},"source":["# Dataset:"]},{"cell_type":"code","metadata":{"id":"W1jChtYq2X-F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"047a315a-ec45-42c6-8bb5-9ae3c7d27faa","executionInfo":{"status":"ok","timestamp":1587043868405,"user_tz":-120,"elapsed":10852,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["batch_size_train = 64\n","batch_size_test = 64\n","# Dataset\n","train_loader, valid_loader, test_loader = get_omniglot_dataloaders_classification(batch_size_train, batch_size_test)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Number of training examples: 211\n","Number of testing examples: 46\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"unGCrDE-6XyL","colab_type":"text"},"source":["# Training:"]},{"cell_type":"markdown","metadata":{"id":"XQF7o5tR2tBU","colab_type":"text"},"source":["## Training parameters:"]},{"cell_type":"code","metadata":{"id":"Qm0A-14h2vjx","colab_type":"code","colab":{}},"source":["# parameters default values\n","epochs = 40\n","lr = 1e-3\n","momentum = 0.5\n","log_interval = 10  # how many batches to wait before logging training status\n","criterion =  F.nll_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KY_9UD34PMD","colab_type":"text"},"source":["## Run No binary Network:"]},{"cell_type":"code","metadata":{"id":"wVJnYyv520FA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1839f5a6-1bae-4923-9c5e-ab003e2ac794","executionInfo":{"status":"ok","timestamp":1587043875710,"user_tz":-120,"elapsed":648,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# parameters model to load no Binary model\n","binary = False\n","\n","model, name_model = get_my_model_Omniglot(binary)\n","print(name_model)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Omniglot_classif_NonBinaryNet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IpnBdJ1q4y1r","colab_type":"code","colab":{}},"source":["path_model_checkpoint = 'trained_models/Omniglot_classif/No_binary_models/'\n","path_save_plot = 'results/Omniglot_results/plot_acc_loss/Omniglot_classif/'\n","\n","run(model, path_model_checkpoint, path_save_plot, name_model, train_loader, valid_loader, epochs, lr, momentum, criterion, log_interval)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NsIF5DJe5xiP","colab_type":"text"},"source":["### Test no binary network:"]},{"cell_type":"code","metadata":{"id":"DEYOtLHL5bEo","colab_type":"code","colab":{}},"source":["# load model pre trained\n","path_model = 'trained_models/Omniglot_classif/No_binary_models/'\n","model.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model)))\n","print(\"Model Loaded\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uiCHm1j6Nc5","colab_type":"code","colab":{}},"source":["evaluate(model, test_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4fLLcYEe6OJZ","colab_type":"text"},"source":["## Run Binary Network:"]},{"cell_type":"code","metadata":{"id":"W3uxyluu6mHH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b952bcef-68b6-43c2-c188-ba00240fcbc2","executionInfo":{"status":"ok","timestamp":1587039111765,"user_tz":-120,"elapsed":791,"user":{"displayName":"Julien Dejasmin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghf77cHAyDw7dPGLWoOwBBO2kQOdHO7YkOXBchE=s64","userId":"11938403868733315090"}}},"source":["# parameters model to load no Binary model\n","binary = True\n","model, name_model = get_my_model_Omniglot(binary)\n","print(name_model)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Omniglot_classif_Stochastic_ST_first_conv_binary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ppaep6VA6q8f","colab_type":"code","colab":{}},"source":["path_model_checkpoint = 'trained_models/Omniglot_classif/Binary_models/'\n","path_save_plot = 'results/Omniglot_results/plot_acc_loss/Omniglot_classif/'\n","\n","run(model, path_model_checkpoint, path_save_plot, name_model, train_loader, valid_loader, epochs, lr, momentum, criterion, log_interval)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHGsUzZL68LH","colab_type":"text"},"source":["### Test binary network:"]},{"cell_type":"code","metadata":{"id":"lyHEXB0v67dr","colab_type":"code","colab":{}},"source":["# load model pre trained\n","path_model = 'trained_models/Omniglot_classif/Binary_models/'\n","model.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model)))\n","print(\"Model Loaded\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VB2lztLi7Cu6","colab_type":"code","colab":{}},"source":["evaluate(model, test_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPVEKJh47GRx","colab_type":"text"},"source":["# Visualization:"]},{"cell_type":"code","metadata":{"id":"tn4n4w8L98Jj","colab_type":"code","colab":{}},"source":["from visualize.viz import visTensor, get_activation, viz_activations, viz_filters\n","from visualize.viz import viz_heatmap,test_predict_few_examples, standardize_and_clip, format_for_plotting\n","from visualize.viz import apply_transforms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WR5XQB5w7Ich","colab_type":"text"},"source":["## Modules:"]},{"cell_type":"code","metadata":{"id":"20WXHNcZ7HaX","colab_type":"code","colab":{}},"source":["def test_predict_few_examples(model):\n","    # classes of fashion mnist dataset\n","    classes = ['0','1','2','3','4','5','6','7','8','9']\n","    # creating iterator for iterating the dataset\n","    dataiter = iter(test_loader)\n","    images, labels = dataiter.next()\n","    images_arr = []\n","    labels_arr = []\n","    pred_arr = []\n","    # moving model to cpu for inference \n","    model.to(\"cpu\")\n","    # iterating on the dataset to predict the output\n","    for i in range(0,10):\n","        images_arr.append(images[i].unsqueeze(0))\n","        labels_arr.append(labels[i].item())\n","        ps = torch.exp(model(images_arr[i]))\n","        ps = ps.data.numpy().squeeze()\n","        pred_arr.append(np.argmax(ps))\n","    # plotting the results\n","    fig = plt.figure(figsize=(25,4))\n","    for i in range(10):\n","        ax = fig.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n","        ax.imshow(images_arr[i].resize_(1, images[0].shape[-1],  images[0].shape[-2]).numpy().squeeze(), cmap='gray')\n","        ax.set_title(\"{} ({})\".format(classes[pred_arr[i]], classes[labels_arr[i]]),\n","                    color=(\"green\" if pred_arr[i]==labels_arr[i] else \"red\"))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzwwjYBE7KXU","colab_type":"text"},"source":["## Load models:"]},{"cell_type":"code","metadata":{"id":"UsTXg9gI7MQP","colab_type":"code","colab":{}},"source":["# parameters model to load no Binary model\n","binary = False\n","model_no_binary, name_model = get_my_model_Omniglot(binary)\n","\n","path_model = 'trained_models/Omniglot_classif/No_binary_models/'\n","if torch.cuda.is_available():\n","  model_no_binary.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model)))\n","else:\n","  model_no_binary.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model), map_location=torch.device('cpu')))\n","print(\"Model Loaded\", name_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_jT45kc7PjX","colab_type":"code","colab":{}},"source":["# parameters model to load no Binary model\n","binary = True\n","model_binary, name_model = get_my_model_Omniglot(binary)\n","\n","path_model = 'trained_models/Omniglot_classif/Binary_models/'\n","if torch.cuda.is_available():\n","  model_binary.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model)))\n","else:\n","  model_binary.load_state_dict(torch.load(fetch_last_checkpoint_model_filename(path_model), map_location=torch.device('cpu')))\n","print(\"Model Loaded\", name_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pX2FBN1I7pfj","colab_type":"text"},"source":["## Visualization few predictions:"]},{"cell_type":"code","metadata":{"id":"jtn4wIVs7vCs","colab_type":"code","colab":{}},"source":["print('No binary model')\n","test_predict_few_examples(model_no_binary, test_loader)\n","plt.show()\n","print('Binary model')\n","test_predict_few_examples(model_binary, test_loader)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lkOtC5l7wWn","colab_type":"text"},"source":["## Visualization Activations values for a specific data:"]},{"cell_type":"code","metadata":{"id":"KNymDQQ_7x0h","colab_type":"code","colab":{}},"source":["index_data = 10\n","viz_activations(model_no_binary, test_loader, index_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObDpCOvn706D","colab_type":"code","colab":{}},"source":["index_data = 10\n","viz_activations(model_no_binary, test_loader, index_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFMzpnsr76-Y","colab_type":"text"},"source":["## Visualization heatmap for a specific data:"]},{"cell_type":"code","metadata":{"id":"MOlggcpn77cO","colab_type":"code","colab":{}},"source":["index_data = 10\n","viz_heatmap(model_no_binary, name_model, test_loader, index_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMAZLPmz78-c","colab_type":"code","colab":{}},"source":["index_data = 10\n","viz_heatmap(model_binary, name_model, test_loader, index_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hvOnHlo7-NP","colab_type":"text"},"source":["## Visualization filters trained:"]},{"cell_type":"code","metadata":{"id":"EE2zLIsT8CHb","colab_type":"code","colab":{}},"source":["viz_filters(model_no_binary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIZ8vU9m8DN3","colab_type":"code","colab":{}},"source":["viz_filters(model_no_binary)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ptDoJsZbRHsg","colab_type":"text"},"source":["## Visualization image that maximizes a specific activation in a specific layer for a specifc filter:"]},{"cell_type":"code","metadata":{"id":"ouwAy7fORIJg","colab_type":"code","colab":{}},"source":["class GradientAscent:\n","    \"\"\"Provides an interface for activation maximization via gradient descent.\n","    This class implements the gradient ascent algorithm in order to perform\n","    activation maximization with convolutional neural networks (CNN).\n","    `Activation maximization <https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf>`_\n","    is one form of feature visualization that allows us to visualize what CNN\n","    filters are \"looking for\", by applying each filter to an input image and\n","    updating the input image so as to maximize the activation of the filter of\n","    interest (i.e. treating it as a gradient ascent task with activation as the\n","    loss). The implementation is inspired by `this demo <https://blog.keras.io/category/demo.html>`_\n","    by Francois Chollet.\n","    Args:\n","        model: A neural network model from `torchvision.models\n","            <https://pytorch.org/docs/stable/torchvision/models.html>`_,\n","            typically without the fully-connected part of the network.\n","            e.g. torchvisions.alexnet(pretrained=True).features\n","        img_size (int, optional, default=224): The size of an input image to be\n","            optimized.\n","        lr (float, optional, default=1.): The step size (or learning rate) of\n","            the gradient ascent.\n","        use_gpu (bool, optional, default=False): Use GPU if set to True and\n","            `torch.cuda.is_available()`.\n","    \"\"\"\n","\n","    ####################\n","    # Public interface #\n","    ####################\n","\n","    def __init__(self, model, img_size=28, lr=1., use_gpu=False):\n","        self.model = model\n","        self._img_size = img_size\n","        self._lr = lr\n","        self._use_gpu = use_gpu\n","\n","        self.num_layers = len(list(self.model.named_children()))\n","        self.activation = None\n","        self.gradients = None\n","\n","        self.handlers = []\n","\n","        self.output = None\n","\n","    @property\n","    def lr(self):\n","        return self._lr\n","\n","    @lr.setter\n","    def lr(self, lr):\n","        self._lr = lr\n","\n","    @property\n","    def img_size(self):\n","        return self._img_size\n","\n","    @img_size.setter\n","    def img_size(self, img_size):\n","        self._img_size = img_size\n","\n","    @property\n","    def use_gpu(self):\n","        return self._use_gpu\n","\n","    @use_gpu.setter\n","    def use_gpu(self, use_gpu):\n","        self._use_gpu = use_gpu\n","\n","    def optimize(self, layer, filter_idx, mean_gradient, ind_x, ind_y, input_=None, num_iter=30):\n","        \"\"\"Generates an image that maximally activates the target filter.\n","        Args:\n","            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n","                which the filter to be chosen, based on `filter_idx`.\n","            filter_idx (int): The index of the target filter.\n","            num_iter (int, optional, default=30): The number of iteration for\n","                the gradient ascent operation.\n","        Returns:\n","            output (list of torch.Tensor): With dimentions\n","                :math:`(num_iter, C, H, W)`. The size of the image is\n","                determined by `img_size` attribute which defaults to 224.\n","        \"\"\"\n","\n","        # Validate the type of the layer\n","\n","        if type(layer) != nn.modules.conv.Conv2d:\n","            raise TypeError('The layer must be nn.modules.conv.Conv2d.')\n","\n","        # Validate filter index\n","\n","        num_total_filters = layer.out_channels\n","        self._validate_filter_idx(num_total_filters, filter_idx)\n","\n","        # Inisialize input (as noise) if not provided\n","\n","        if input_ is None:\n","            input_ = np.uint8(np.random.uniform(\n","                150, 180, (self._img_size, self._img_size, 1)))\n","            input_ = apply_transforms(input_, size=self._img_size)\n","\n","        if torch.cuda.is_available() and self.use_gpu:\n","            self.model = self.model.to('cuda')\n","            input_ = input_.to('cuda')\n","\n","        # Remove previous hooks if any\n","\n","        while len(self.handlers) > 0:\n","            self.handlers.pop().remove()\n","\n","        # Register hooks to record activation and gradients\n","\n","        self.handlers.append(self._register_forward_hooks(layer, filter_idx, mean_gradient, ind_x, ind_y))\n","        self.handlers.append(self._register_backward_hooks())\n","\n","        # Inisialize gradients\n","\n","        self.gradients = torch.zeros(input_.shape)\n","\n","        # Optimize\n","\n","        return self._ascent(input_, num_iter)\n","\n","    def visualize(self, layer, filter_idxs=None, mean_gradient=True, ind_x=None, ind_y=None,\n","                  lr=1., num_iter=30,\n","                  num_subplots=4, figsize=(4, 4), title='Conv2d',\n","                  return_output=False):\n","        \"\"\"Optimizes for the target layer/filter and visualizes the output.\n","        A method that combines optimization and visualization. There are\n","        mainly 3 types of operations, given a target layer:\n","        1. If `filter_idxs` is provided as an integer, it optimizes for the\n","            filter specified and plots the output.\n","        2. If `filter_idxs` is provided as a list of integers, it optimizes for\n","            all the filters specified and plots the output.\n","        3. if `filter_idx` is not provided, i.e. None, it randomly chooses\n","            `num_subplots` number of filters from the layer provided and\n","            plots the output.\n","        It also returns the output of the optimization, if specified with\n","        `return_output=True`.\n","        Args:\n","            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n","                which the filter to be chosen, based on `filter_idx`.\n","            filter_idxs (int or list of int, optional, default=None): The index\n","                or indecies of the target filter(s).\n","            lr (float, optional, default=.1): The step size of optimization.\n","            num_iter (int, optional, default=30): The number of iteration for\n","                the gradient ascent operation.\n","            num_subplots (int, optional, default=4): The number of filters to\n","                optimize for and visualize. Relevant in case 3 above.\n","            figsize (tuple, optional, default=(4, 4)): The size of the plot.\n","                Relevant in case 1 above.\n","            title (str, optional default='Conv2d'): The title of the plot.\n","            return_output (bool, optional, default=False): Returns the\n","                output(s) of optimization if set to True.\n","        Returns:\n","            For a single optimization (i.e. case 1 above):\n","                output (list of torch.Tensor): With dimentions\n","                    :math:`(num_iter, C, H, W)`. The size of the image is\n","                    determined by `img_size` attribute which defaults to 224.\n","            For multiple optimization (i.e. case 2 or 3 above):\n","                output (list of list of torch.Tensor): With dimentions\n","                    :math:`(num_subplots, num_iter, C, H, W)`. The size of the\n","                    image is determined by `img_size` attribute which defaults\n","                    to 224.\n","        \"\"\"\n","\n","        self._lr = lr\n","        self.mean_gradient = mean_gradient\n","        self.ind_x = ind_x\n","        self.ind_y = ind_y\n","\n","        if not self.mean_gradient:\n","          assert self.ind_x != None and self.ind_y != None, 'if mean_gradient is false, you must choice x and y index'\n","\n","\n","        if (type(filter_idxs) == int):\n","            output = self._visualize_filter(layer,\n","                                            filter_idxs,\n","                                            self.mean_gradient,\n","                                            self.ind_x,\n","                                            self.ind_y,\n","                                            num_iter=num_iter,\n","                                            figsize=figsize,\n","                                            title=title)\n","        else:\n","            num_total_filters = layer.out_channels\n","\n","            if filter_idxs is None:\n","                num_subplots = min(num_total_filters, num_subplots)\n","                filter_idxs = np.random.choice(range(num_total_filters),\n","                                               size=num_subplots)\n","\n","            self._visualize_filters(layer,\n","                                    filter_idxs,\n","                                    self.mean_gradient,\n","                                    self.ind_x,\n","                                    self.ind_y,\n","                                    num_iter,\n","                                    len(filter_idxs),\n","                                    title=title)\n","\n","        if return_output:\n","            return self.output\n","\n","    #####################\n","    # Private interface #\n","    #####################\n","\n","    def _register_forward_hooks(self, layer, filter_idx, mean_gradient, ind_x, ind_y):\n","          def _record_activation(module, input_, output):\n","              if mean_gradient:\n","                  # maximization of mean for filter_idx\n","                  self.activation = torch.mean(output[:,filter_idx,:,:])\n","              else:\n","                  # maximization of a specific neuron for filter_idx\n","                  self.activation = output[:,filter_idx,ind_x,ind_y]\n","          return layer.register_forward_hook(_record_activation)\n","\n","    def _register_backward_hooks(self):\n","        def _record_gradients(module, grad_in, grad_out):\n","            if self.gradients.shape == grad_in[0].shape:\n","                self.gradients = grad_in[0]\n","\n","        for _, module in self.model.named_modules():\n","            if isinstance(module, nn.modules.conv.Conv2d) and \\\n","                    module.in_channels == 1:\n","                return module.register_backward_hook(_record_gradients)\n","\n","    def _ascent(self, x, num_iter):\n","        output = []\n","\n","        for i in range(num_iter):\n","            self.model(x)\n","            self.activation.backward()\n","            self.gradients /= (torch.sqrt(torch.mean(\n","                torch.mul(self.gradients, self.gradients))) + 1e-5)\n","            x = x + self.gradients * self._lr\n","            output.append(x)\n","            # TODO: regarder loss et acc pour voir si Ã§a fonctionne\n","\n","        return output\n","\n","    def _validate_filter_idx(self, num_filters, filter_idx):\n","        if not np.issubdtype(type(filter_idx), np.integer):\n","            raise TypeError('Indecies must be integers.')\n","        elif (filter_idx < 0) or (filter_idx > num_filters):\n","            raise ValueError(f'Filter index must be between 0 and {num_filters - 1}.')\n","\n","    def _visualize_filter(self, layer, filter_idx, mean_gradient, ind_x, ind_y, num_iter, figsize, title):\n","        self.output = self.optimize(layer, filter_idx, mean_gradient, ind_x, ind_y, num_iter=num_iter)\n","        \n","        plt.figure(figsize=figsize)\n","        plt.axis('off')\n","        plt.title(title)\n","        \n","        plt.imshow(format_for_plotting(\n","            standardize_and_clip(self.output[-1],\n","                                 saturation=0.15,\n","                                 brightness=0.7)), cmap='gray');\n","    \n","        plt.show()\n","        # plt.imsave('plot_image_maximize_filter_layer2_model_MNIST.png')\n","\n","    def _visualize_filters(self, layer, filter_idxs, mean_gradient, ind_x, ind_y, num_iter, num_subplots,\n","                           title):\n","        # Prepare the main plot\n","\n","        num_cols = 4\n","        num_rows = int(np.ceil(num_subplots / num_cols))\n","\n","        fig = plt.figure(figsize=(16, num_rows * 5))\n","        plt.title(title)\n","        plt.axis('off')\n","        \n","\n","        self.output = []\n","\n","        # Plot subplots\n","        for i, filter_idx in enumerate(filter_idxs):\n","            output = self.optimize(layer, filter_idx, mean_gradient, ind_x, ind_y, num_iter=num_iter)\n","\n","            self.output.append(output)\n","\n","            ax = fig.add_subplot(num_rows, num_cols, i+1)\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","            ax.set_title(f'filter {filter_idx}')\n","\n","            \n","            ax.imshow(format_for_plotting(\n","                standardize_and_clip(output[-1],\n","                                     saturation=0.15,\n","                                     brightness=0.7)), cmap='gray')\n","        plt.subplots_adjust(wspace=0, hspace=0);\n","        # plt.imsave('plot_image_maximize_filter_layer2_model_MNIST.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XxHHpctSRMM_","colab_type":"text"},"source":["### No binary model:"]},{"cell_type":"code","metadata":{"id":"ivGzA9TbRI4A","colab_type":"code","colab":{}},"source":["g_ascent_no_binary = GradientAscent(model_no_binary)\n","g_ascent_no_binary.use_gpu = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7Q6qHlgRaQl","colab_type":"code","colab":{}},"source":["conv1_no_binary = model_no_binary.layer1\n","conv1_filters_no_binary = [0,1,2,3,4,5,6,7,8,9]\n","mean_gradient_layer1 = False\n","ind_x_layer1 = 14\n","ind_y_layer1 = 14\n","\n","conv2_no_binary = model_no_binary.layer2\n","conv2_filters_no_binary = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n","mean_gradient_layer2 = False\n","ind_x_layer2 = 6\n","ind_y_layer2 = 6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3qlGx5rRcHa","colab_type":"code","colab":{}},"source":["g_ascent_no_binary.visualize(conv1_no_binary, conv1_filters_no_binary, mean_gradient_layer1,\n","                             ind_x_layer1, ind_y_layer1, title='No binary model: conv layer 1')\n","g_ascent_no_binary.visualize(conv2_no_binary, conv2_filters_no_binary, mean_gradient_layer2,\n","                             ind_x_layer2, ind_y_layer2, title='No binary model: conv layer 2')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34cZe9oDRcpx","colab_type":"text"},"source":["### Binary model:"]},{"cell_type":"code","metadata":{"id":"VzMRUrXhRhCM","colab_type":"code","colab":{}},"source":["g_ascent_binary = GradientAscent(model_binary)\n","g_ascent_binary.use_gpu = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKi3LO71RjEj","colab_type":"code","colab":{}},"source":["conv1_binary = model_binary.layer1\n","conv1_filters_binary = [0,1,2,3,4,5,6,7,8,9]\n","mean_gradient_layer1 = False\n","ind_x_layer1 = 14\n","ind_y_layer1 = 14\n","\n","conv2_binary = model_binary.layer2\n","conv2_filters_binary = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n","mean_gradient_layer2 = False\n","ind_x_layer2 = 6\n","ind_y_layer2 = 6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKu580D5Rjy7","colab_type":"code","colab":{}},"source":["g_ascent_binary.visualize(conv1_binary, conv1_filters_binary, mean_gradient_layer1,\n","                             ind_x_layer1, ind_y_layer1, title='Binary model: conv layer 1')\n","g_ascent_binary.visualize(conv2_binary, conv2_filters_binary, mean_gradient_layer2,\n","                             ind_x_layer2, ind_y_layer2, title='Binary model: conv layer 2')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VxetH987RkXq","colab_type":"text"},"source":["## Visuazation regions that maximizes a specific layer and filter:"]},{"cell_type":"code","metadata":{"id":"enaMuOPIRlxA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}